\documentclass[twocolumn]{article}
\usepackage{pgfplots}
\usepackage{subfig}
\usepackage{amsmath}
\usepackage{verbatim}
\usepgfplotslibrary{statistics}
\usepgfplotslibrary{groupplots}
\pgfplotsset{compat=1.13}

  \title{Fast Search on Small, Uniformly Distributed Arrays}
  \date{\today}
  \author{Peter Van Sandt \and Jignesh Patel}

\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pgfplotsset{select coords between index/.style 2 args={
		x filter/.code={
			\ifnum\coordindex<#1\def\pgfmathresult{}\fi
			\ifnum\coordindex>#2\def\pgfmathresult{}\fi
		}
	}}

  \onecolumn
\maketitle
\begin{center}
University of Wisconsin-Madison \\
\{van-sandt, jignesh\}@cs.wisc.edu
\end{center}

\begin{center}
\textbf{Abstract}
\end{center}

In this report, we demonstrate interpolation search can be much more effective than binary search on a uniformly distributed dataset. We provide techniques that can be applied to either search algorithm to increase performance and justification for why these techniques work. We guide the decision between binary and interpolation search with metrics that predict the performance of interpolation search.

We compare the performance of binary and interpolation search across several dataset sizes, and we give a variation of interpolation search that outperforms binary search on every measured size by adapting the recursion depth to its effectiveness on that dataset size. We also give an optimized version of interpolation-sequential search that outperforms binary search by 2.3x. Finally, we demonstrate that these algorithms and techniques maintain their effectiveness when many threads are thrashing the caches with concurrent searches.

\twocolumn
\section{Introduction}
\label{introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Search is a well-studied problem, but applying a few optimization techniques can exploit modern hardware for significant performance gain. We seek to understand how modern hardware has changed which algorithms are best, and how to implement them to maximize performance. The benefits of the lower asymptotic complexity of interpolation search relative to binary search has been well studied in the context of data sets that require going to external memory. \cite{manolopoulos-kollias-burton} Interactive data visualization platforms have made in-memory workloads increasingly important, and they call for a re-evaluation of the performance of interpolation search on in-memory datasets in the context of modern processors.

Modern processors now support the execution of multiple operations per cycle, instruction-level parallelism (ILP), and have reduced the cost of multiplication, floating point operations, and reading from cache. This change in platform suggests the importance of revisiting which search algorithms results in lowest running time. Compilers are capable of a great deal of optimization techniques, but they can't help if your code isn't structured to encourage them. We will focus on techniques like adding loop prologues which change the algorithm or re-structure the code to encourage the compiler to use more low level optimizations like loop unrolling.

There are three primary ways to improve search. First, find the element with fewer probes. This is the goal of interpolation search. Second, reduce the cost of each iteration. Finally, increase the degree to which each iteration can overlap with the last with ILP. The following optimizations focus on cost reduction and increasing ILP. We use Compiler Explorer to analyze how C++ source code turns into assembly code, and we use the Intel Architecture Code Analyzer to understand how that assembly code interacts with the test Haswell processor.

%These are very related, to have multiple iterations in flight simultaneously, each iteration must be pretty small, but they are distinct because 

Binary search is the defacto fastest method for searching any array. In every iteration, itreduces the remaining interval that could contain the target by half. We introduce $b-lr$ in figure \ref{b-lr-code} which tracks this interval with the leftmost and rightmost indexes. In $b-sz$, see figure \ref{b-sz-code}, we track this interval with the leftmost index and the remaining size of the interval. Many of the optimizations we consider apply more generally, but the choice of how to track this interval does not have a clear best answer, and each general technique interacts with the tracking of this interval in subtly different ways. 

Interpolation search approximates the expected position of the target by interpolating between the leftmost and rightmost elements. This assumes a linear relationship between the values of the sorted array and their positions as when the values are chosen uniformly at random. Because interpolation search uses more expensive arithmetic to reduce the number of probes, (key comparisons) it is traditionally considered in the context of searching ordered files where arithmetic overhead is less significant relative to file IO.

Perl, Itai, and Avni \cite{perl-itai-avni} show that interpolation search will take $O(\log \log N)$ probes on a uniformly distributed keys with high probability. Gonnet and Rogers \cite{gonnet-rogers} give an analysis of an interpolation-sequential search that uses a single interpolation to guide a subsequent linear search. They show that the expected number of probes is $O(\sqrt N)$. The asymptotic analysis of binary search at $O(\log N)$ lies in between these two bounds, but we will concern ourselves with the running time which includes constant factors.

\begin{figure}[ht]
\begin{verbatim}
def b-lr(x, A):
  left = 0
  right = len(A)
  while right > left:
    mid = left + (right-left) / 2
    if A[mid] < x:
      left = mid + 1
    elif A[mid] > x:
      right = mid 
    else:
      return A[mid]
  return A[left]
\end{verbatim}
\caption{b-lr}
  \label{b-lr-code}
\end{figure}

\begin{figure}[ht]
\begin{verbatim}
def b-sz(x, A):
  left = 0
  n = len(A)
  while n > 1:
    half = n / 2
    if A[left + half] > x:
      n = half
    elif A[left + half] < x:
      left = left + half + 1
      n = n - half - 1
    else:
      return A[left + half]
  return A[left]
\end{verbatim}
\caption{b-sz}
  \label{b-sz-code}
\end{figure}

\begin{figure}[ht]
\begin{verbatim}
def i(x, A):
  left = 0
  right = len(A) - 1
  while left < right:
    width_range = (right - left) /
      (A[right] - A[left])
    mid = left = (x - A[left]) *
      width_range
    if A[mid] < x:
      left = mid + 1
    elif x < A[mid]:
      right = mid - 1
    else
      return A[mid]
  return A[left]
\end{verbatim}
\caption{i}
  \label{i-code}
\end{figure}

\section{Testing Methodology}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Each dataset is populated with random 8 byte integers and sorted. To represent the ideal case for an interpolation search, each dataset was generated from a uniform distribution. To include the dependence of interpolation search on the dataset distribution, several seeds were randomly chosen to generate each dataset and each algorithm was evaluated on each dataset.

We choose a random permutation of each dataset to fix the search ordering across each search algorithm. The randomization of the ordering is to reduce the impact of branch prediction or other ordering-related hardware mechanisms that might distort the evaluation. Because the average search time is in the tens of nanoseconds on small arrays, it's not feasible to sample on every search. In the case of a larger array, storing the sample timings might fill the cache, distorting the results. So, we measure the time to search for every element in the dataset in a random permutation, and then collect multiple such samples.

All samples for a given algorithm are concatenated together and we report the median and the upper and lower quartile values. This simplifies the presentation, but means that some of the variation represented is due to variation in the average search time on a given dataset, and most of it is variation in the average search time between datasets. Because interpolation search relies more heavily on ILP mechanisms, it is more vulnerable to noise from the hardware. Because interpolation search executes based on the distribution of the dataset, there is a great deal of variation in performance between datasets.

All benchmarks were compiled using clang 5.0.1 on maximum optimization settings including unsafe floating point math, and run on an i5-4570 CPU at 3.2Ghz with 32K L1 data and instruction caches, 256K L2 cache, and 6144K L3 cache. 


\section{Optimizations}

\begin{figure}[t]
	\begin{tikzpicture}
	\begin{axis}[
	small,
	width=\columnwidth,
	height=\textheight / 4,
	nodes near coords,
	nodes near coords align={vertical},
	enlarge y limits={.2},
	xtick=data,
	xticklabel style={rotate=90, font=\boldmath},
	xlabel near ticks,
	ybar,
	title={$b-lr$ Optimizations Summary},
	xlabel=Variation,
	ylabel={Normalized Improvement},
	xticklabels from table={b-lr-improve.dat}{alg}
	]
	\addplot plot [error bars/.cd, y dir=both, y explicit] table[x expr=\coordindex, y=median, y error plus=plus25, y error minus=minus25] {b-lr-improve.dat};
	\end{axis}
	\end{tikzpicture}
\end{figure} \label{b-lr}

\begin{figure}[t]
	\begin{tikzpicture}
	\begin{axis}[
	small,
	width=\columnwidth,
	height=\textheight / 4,
	nodes near coords,
	nodes near coords align={vertical},
	enlarge y limits={.2},
	xtick=data,
	xticklabel style={rotate=90, font=\boldmath},
	xlabel near ticks,
	ybar,
	title={$b-sz$ Optimizations Summary},
	xlabel=Variation,
	ylabel={Normalized Improvement},
	xticklabels from table={b-sz-improve.dat}{alg}
	]
	\addplot plot [error bars/.cd, y dir=both, y explicit] table[x expr=\coordindex, y=median, y error plus=plus25, y error minus=minus25] {b-sz-improve.dat};
	\end{axis}
	\end{tikzpicture}
\end{figure} \label{b-sz}

\begin{figure}[t]
	\begin{tikzpicture}
	\begin{axis}[
	small,
	width=\columnwidth,
	height=\textheight / 4,
	nodes near coords,
	nodes near coords align={vertical},
	enlarge y limits={.2},
	xtick=data,
	xticklabel style={rotate=90, font=\boldmath},
	xlabel near ticks,
	ybar,
	title={Interpolation Optimizations Summary},
	xlabel=Variation,
	ylabel={Normalized Improvement},
	xticklabels from table={i-improve.dat}{alg}
	]
	\addplot plot [error bars/.cd, y dir=both, y explicit] table[x expr=\coordindex, y=median, y error plus=plus25, y error minus=minus25] {i-improve.dat};
	\end{axis}
	\end{tikzpicture}
\end{figure} \label{i}

\subsection{Use One Exit}
\label{exit}
$b-lr$ returns as soon as it discovers that the probe hit the target. Because we require it to jump out of the loop at that point rather than setting the exit condition, we have to apply a compare and branch in the middle of the loop. We introduce $b-lr-cond$ which instead, sets the exit condition to force the loop to exit on the next iteration. This reduces the number of exit conditions from two to one and allows for the body of the loop to be in terms of branches, rather than conidtional moves. It also simplifies the structure of the loop enhancing future modifications. $b-sz-cond$ has a similar modification, but clang appears to already have good insight into the function and is employing many optimization techniques that we have to re-organize $b-lr$ to get.

\subsection{Overflow Math}
\label{overflow}
If we calculate the midpoint for $b-lr$ by choosing $mid = \frac{left+right}{2}$, we run the risk of overflow which Khuong notes as unsafe. \cite{pvk-search-retro} While the formula given in figure \ref{b-lr-code} will never overflow for any representable value of $n$, it is more expensive. We introduce $b-lr-over$ which uses the cheaper formula and risks overflow to calculate the midpoint, but we show that overflow is possible only for unusually large arrays. This is necessary to set up the full benefits from future optimizations even if the direct effect is to change the location of a subtraove instruction.

Let $max$ be the maximum value. Overflow is defined by \eqref{overflow:1}. By the definition of binary search, we can bound $left$ and $right$ by $n$ in \eqref{overflow:2}. The sum of $left$ and $right$ is maximized in \eqref{overflow:3}. For this choice of $left$ and $right$, we can see by \eqref{overflow:1} and \eqref{overflow:3} that $n$ overflows only if \eqref{overflow:4} holds. Because $left$ and $right$ are maximized, this is the minimum value of $n$ where overflow can occur. In \eqref{overflow:5} we have the representable values for which the equation used in $b-lr-over$ can overflow. For 64-bit integer arithmetic, we can simplify this to \eqref{overflow:6}. Since this is such an unusual array size, we think it's a pre-condition worth making.

\begin{align}
left + right \geq 1 + max \label{overflow:1}
\\ left < right \leq n \label{overflow:2}
\\ left + 1 = right = n \label{overflow:3}
\\ n \geq 1 + \frac{1}{2}max \label{overflow:4}
\\ \frac{1}{2}max < n < 1 + max \label{overflow:5}
\\ 2^{63} < n < 2^{64} \label{overflow:6}
\end{align}

\subsection{Don't Test Equality}
\label{noeq}
Because $b-lr-over$ doesn't return early, it is already implemented solely in terms of conditional moves. We introduce $b-lr-noeq$ and $b-sz-noeq$ which eliminates a check for equality, reducing the number of conditional moves and saving an increment.. This accelerates the body of the loop at the expense of requiring additional iterations. This may result in a worse running time for the first iteration, but the code simplification and additional structure is necessary to get the full benefit from the other optimizations. In interpolation, we tried variants that did an early equality check and others that relied on properties of interpolation to exit early, and we found that including the equality test was a net benefit.

One of the primary benefits of removing the test for equality is that the number of iterations no longer varies, so the size of the range always decreases by the same amount, $n_2 = n_1 - \lfloor \frac{1}{2}n_1 \rfloor$. This additional structure in the loop allows for more optimizations such as loop unrolling, and replacing conditional moves with simple branches. After removing the checks for equality, the number of iterations in interpolation search still varies, so it does not benefit in the same way. Even with a fixed number of iterations, the added code size of interpolation search reduces the effectiveness of this additional structure. Instead, because each iteration of interpolation search requires more work than an interation of binary search, so reducing the number of iterations of interpolation search yields more benefits than making each iteration faster.

This optimization has a surprising dependence on the overflow math optimization. If the overflow math optimization is used while testing for equality, three conditional moves are needed. If we don't test for equality, but we choose the more expensive equation, then three conditional moves are needed. Only by combining the two do we see the saved conditional move. This still works out to be worse, but is a set up for loop unrolling in the with indexed for loops.

Conditional moves are sometimes faster than simple branches and sometimes slower depending on the surrounding code and how much ILP is available. Carruth \cite{carruth} goes into some of the advantages and disadvantages of conditional moves over branches. The basic tradeoff is that conditional moves require flushing the pipeline and branches run the risk of cycles lost to misprediction, but I won't go into too much detail here.

$b-sz$ and $b-sz-cond$ are faster than the first few variations of binary search that do not check for equality because clang was able to unroll their loops. We will eventually be able to structure the variations that don't check for equality to achieve similar optimizations to the ones that do check for equality, and we will be able to take those variations farther.

\subsection{Unroll with For Loops}
The number of iterations of a binary search can be upper bounded by the ceiling of the binary logarithm of the size, so that can be precomputed. We introduce $b-lr-for$, $b-lr-noeq-for$, $b-sz-for$, and $b-sz-noeq-for$ which, instead of checking the size of the remaining interval directly, use an indexed for loop with a logarithmic number of iterations.

An indexed for loop isn't inherently better, but clang did not do any run-time loop unrolling on the while loop for $b-lr-noeq$ even though some level of prediction could be done based on the distance. Clang did unrolling on the indexed for loop, which in combination with an optimized loop body increases the number of iterations that can be in-progress simultaneously. We only tested the version that precomputes the logarithmic bound, and we didn't attempt to manually unroll the while loop by conditioning on a remaining interval size of $2^n$ and manually unrolling $n$ such iterations.

Note that this optimization gets much of its benefit from interacting with other optimizations. $b-lr-for$ and $b-lr-noeq$ are worse than $b-lr-cond$ when taken individually. However, when these two are combined, $b-lr-noeq-for$ is fastest. This also has an interaction with the avoiding multiple exit points. If there is an early return in the body of the loop, then the compiler doesn't unroll the loop and performance is worse than not using the for loop at all.

To benefit from loop unrolling, you must ensure that the number of iterations performed exceeds the unroll factor. We found that a more complicated midpoint calculation performed better than a simple one when the binary search required fewer iterations, and it performed better when binary search required more iterations. We discovered this was because the simpler loop was unrolled more times than the complex version, so shorter searches could only benefit from unrolling with the complicated loop. Initially, we thought we observed the complicated loop to be faster, instead we were observing loop unrolling. We observed the fastest loop had the body of the simpler loop and the unroll factor of the complicated one.


\subsection{Precomputation}
\label{precomputation}
Precomputation is done in binary search by calculating the maximum number of iterations until the target is found, which we discuss in section \label{for}. We introduce $i-precompute$, a variation of interpolation search which pre-computes the slope of the first interpolation. Since the next interpolation calculation depends on the result of the previous, the sooner we can start to load the interpolated value which is on critical path through the function. In this way, we replace 3 loads and a division with a single load and multiplication, reducing the non-parallelizable part of the function.

\subsection{Loop Prologues}
\label{prologues}
We believe that it is common for the first iteration of a loop to require additional work or checks that aren't required in later iterations of the loop. $b-sz$ as stated does not appear to have this form, but every iteration requires rounding up the size of the remaining interval. We introduce $b-sz-pow$ which reduces the interval to a power of two in the first iteration. See figure \ref{b-sz-pow} for an example of how we do this. Once we have established that the target is contained within an interval with a size that's a power of two, we do not need to do any more ceiling functions, which saves a subtraction in the critical path.

Later, we will discuss the usage of linear search as a base case and in an interpolation-sequential search. We also applied a loop prologue to accelerate a linear search with SIMD. (Single Instrution, Multiple Data instructions process many elements with a single instruction.) There is a dramatic difference in performance between aligned load instructions, which will fault if the address isn't aligned, and unaligned load instructions. Since an interpolation can happen at an arbitrary address, alignment isn't guaranteed. However, if you use a prologue of a standard, unrolled, linear search that covers the width of the needed alignemnt, then you know that starting at the first aligned position after in the interpolation isn't missing any elements. Additionally, since there is some overhead in communicating with vector processors, which allows the CPU to speculate on the linear search and terminate faster than it would if it had used unaligned vector instructions for the same task. 

\begin{figure}[ht]
\begin{verbatim}
def b-sz-pow(x, A):
  left = 0
  n = len(A)
  mid = n - 2**(ceil_lg_n - 1)
  left = mid if A[mid] <= x else left
  n -= mid
  for i in range(1, ceil_lg_n):
    n /= 2
    left = left + n if A[left + n] <= x else left
  return A[left]
\end{verbatim}
\caption{b-sz-pow}
  \label{b-sz-pow}
\end{figure}

\subsection{Linear Search Base Case}
\label{linear}
Linear search trades asymptotic efficiency for the ability to probe multiple elements per cycle. We analyzed the throughput of my unrolled linear search and an optimized binary search with IACA, Intel's architecture analyzer. We found that the unrolled linear search could probe two elements every cycle at full throughput, while the optimized binary search would probe one element every other cycle. This is before accounting for branch mispredictions which are guaranteed to be common with binary search and uncommon with linear search. Binary search can't reliably execute its loads speculatively because which element to load depends on the previous comparison, so it is unlikely to be able to correctly schedule very many loads in advance. Linear search, however, will always load the next element in the array, so it will effectively hide much of the memory latency.

While binary search and interpolation search have an asymptotic advantage per iteration, because linear search can execute more iterations in the same amount of time, it is superior for sufficiently small sizes. This difference is even more dramatic for interpolation search, where each iteration is more expensive.

We introduce $b-lr-lin$ and $b-sz-lin$ which use linear search as a base case for binary search. We use a binary search to reduce the remaining interval to 32 elements. Then, we use the middle element of the remaining interval to do a linear search in the direction of the target element from there. We introduce $i-guard$ which does a recursive interpolation search like $i-precompute$, but if it detects that the interpolation is on the boundaries of the remaining interval, then it will simply do a linear search from that point. This detects when interpolation search behaves like a linear search, and uses a linear search as a much less expensive base case. All other interpolation searches use a similar mechanism, but they use only the outermost element as the boundary of the remaining interval.

Finally, we introduce $i-seq-fp$ as a final way of exploiting linear search as a base case. Gonnet and Rogers [1977] give an analysis of interpolation-sequential search which does a single interpolation followed by a linear search in the direction of the target. This algorithm can be combined with pre-computation to completely avoid any of the expensive operations involved in interpolation, instead requiring only a subtraction and a multiplication. This is the most significant impact of the pre-computation optimization.

The principle behind why the interpolation-sequential search works is that the target key is likeliest to be nearby the interpolation. An interpolation may not reduce the remaining interval by as much as a binary search does, the evaluation is likely to be much closer. A binary search on the remaining interval is not as effective as a linear search from the interpolation point at exploiting this locality.

\subsection{Lookup Table Division}
\label{lut}
Lookup table division is a form of precomputation that sacrifices some accuracy in order to reduce the amount of conversions to and from floating point, as well as to trade off some expensive operations for some cheaper ones. Recursive interpolation search relies upon a large number of dependent divisions. Because the linear interpolation is itself not accurate with respect to positions within the array, we are willing to forgo some accuracy in the divisions themselves to make each division faster. Most of the accuracy of a division comes from its most significant bits and its magnitude, so we build a table of magic constants which allow us to approximate division by a small number of factors. We introduce $i-lut$ and $i-seq$, which are variations that replace the floating-point division of their counterparts $i-precompute$ and $i-seq-fp$ respectively with lookup table division.

We find these magic constants by dividing $2^63$ by the desired divisor, multiply by two, and save the result constant in a lookup table. The x86 multiplication operation yields both the high and the low word results. To approximate division, simply multiply by this constant and take the high word. In the loop, we use some bit counting and other such simple operations to determine how much shifting is needed to do the approximation and lookup. These turn out to be the bulk of the work in the division. This method is similar to the method used by compilers to replace integer division, but we sacrifice some accuracy to make it cheaper. Granlund and Montgomery \cite{granlund-montgomery} provide the method and its analysis, and ridiculousfish \cite{fish} explains the concept and provides some intuition for why it works.

In order to perform the lookup, we have to reduce the divisor into the range of the lookup table. On smaller arrays, this loss of precision is outweighed by increased performance, however, on larger datasets, the higher precision of floating point division is more important. We don't have to pay for the lookup of the first interpolation, however, so the first interpolation requires exactly one multiplication.

I accelerated optimized variations of linear search and jump search with SIMD instructions, but I found no variation of binary or interpolation search that used these base case algorithms to be superior to any implementation that simply used an unrolled for loop.

\setlength{\tabcolsep}{5pt}
\begin{figure}[ht]
  \caption{Summary of Variation and Optimizations}
\begin{tabular}{l*{9}c}
  Variation & \S & X & M & Q & F & C & P & L & T \\
	\hline
  b-lr & \ref{introduction} \\
  b-lr-cond & \ref{exit} & y \\
  b-lr-over & \ref{overflow} & y & y \\
  b-lr-noeq & \ref{noeq} & y & y & y \\
  b-lr-for & \ref{for} & y & y & & y & y \\
  b-lr-noeq-for & \ref{for} & y & y & y & y & y \\
  b-lr-lin & \ref{linear} & y & y & y & y & y & & y & \\

  b-sz & \ref{introduction} \\
  b-sz-cond & \ref{exit} & y \\
  b-sz-noeq & \ref{noeq} & y & y & y \\
  b-sz-for & \ref{for} & y & y & & y & y \\
  b-sz-noeq-for & \ref{for} & y & y & y & y & y \\
  b-sz-pow & \ref{prologues} & y & y & y & y & y & y \\
  b-sz-lin & \ref{linear} & y & y & y & y & y & y & y & \\
	
  i & \ref{introduction} \\
  i-precompute & \ref{precomputation} & & & & & y \\
  i-lut        & \ref{lut} & & & & & y & & & y \\
  i-seq-fp     & \ref{linear} & & & & & y & y & y \\
  i-seq        & \ref{lut} & & & & & y & y & y & y \\
  i-guard     & \ref{linear} & & & & & y & y & y \\
\end{tabular}
  \caption{one eXit, overflow Math, don't check eQuality, indexed For loops, preComputation, Prologue, Linear search, lookup Table division}
  \label{table}
\end{figure}

Some variations discussed build on others. Some try different combinations of optimizations, and still others can't be described by the optimizations discussed. The table in figure \ref{table} summarizes each variant and the optimizations involved. We summarize the improvement of each optimization over the naive implementtion in figures \ref{b-lr}, \ref{b-sz}, and \ref{i}. Since the compiler was more successful in optimizing the naive version of $b-sz$, it shows less improvement.

\section{Predictors of Interpolation Performance}
\begin{figure}[t]
	\begin{tikzpicture}
	\begin{axis}[
		ybar,
		ymin=0,
	title={Interpolation Error},
	xlabel={Distance},
	ylabel={Count}
	]
	\addplot +[
	hist={
		cumulative,
		density,
		bins=10
	}] table [y index=0] {cdf.dat};
	\end{axis}
	\end{tikzpicture}
\end{figure} \label{cdf}

The performance of interpolation search depends upon the distribution of the values in the array, however, on the arrays that we have filled with numbers chosen uniformly at random, interpolation search performs much better than binary search. So, if we can determine which characteristics of a dataset most strongly predict the performance of interpolation search on that dataset, that information might be used to decide which kind of search to use.

We wanted to represent how closely an interpolation could represent a typical array that fits the target distribution, so figure \ref{cdf} shows the CDF of the 1000 element dataset that yielded the median performance on $i-lin$. It shows that most elements are very close to the interpolation but that thetail is somewhat long.

For each element in the data, we can consider the index of where that element would first interpolate to in relationship to where that element actually is. Using this distance, we consider the ninetieth percentile, maximum, and mean absolute distances. ($L_1$ norm.) We also consider if the $R^2$ value of a line of best fit to be predictive of the performance of an algorithm that depends upon the data being well represented by a line. Finally, we can consider smoothness as used by Demaine, Jones, and Patrascu to build a search tree that generalizes interpolation search. \cite{demaine-jones-patrascu}

Our analysis in figure \ref{predict} shows that the mean absolute distance predicts the running time of interpolation search on this dataset nearly perfectly. Other metrics that are defined by the interpolation error perform significantly better than metrics that are defined by some additional factor which aren't predictive of the data. We also show the raw values for the best predictor, the mean absolute distance. So, low mean absolute distance suggests that interpolation search may be a good choice whereas a higher value suggests that a binary search may be preferable.


\begin{figure}[t]
	\begin{tikzpicture}
	\begin{axis}[
	xtick=data,
	title={Metric Prediction of Performance},
	xlabel=Metric,ylabel={$R^2$},
	xticklabels from table={predict.dat}{metric},
	ybar
	]
	\addplot table[x expr=\coordindex, y=r2] {predict.dat};
	\end{axis}
	\end{tikzpicture}
\end{figure} \label{predict}

\begin{figure}[t]
	\begin{tikzpicture}
	\begin{axis}[
	title={Best Predictor of Performance},
    ylabel={Average Search Time (ns)},
    xlabel={Mean Absolute Average Distance}
	]
	\addplot table[only marks, x=l1, y=ns] {metrics.dat};
	\end{axis}
	\end{tikzpicture}
\end{figure}

\section{Binary vs Interpolation Search}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

With optimizations that reduce the constant factor overheads, an interpolation search that adapts to the size of the dataset out-performs binary search on all measured dataset, and its asymptotic superiority causes this advantage to increase with the size of the dataset. An interpolation-sequential search shows the best performance when searching one thousand elements, but is hindered by its $O(\sqrt N)$ asymptotic complexity as the dataset grows. Interpolation-sequential search scales more favorably, although not as well as recursive interpolation search when the sequential search is accelerated with vector instructions, but it also is worse than the simpler linear search on smaller datasets.

I found a clear winner for binary search that outperforms all other variations on all datasets and array sizes, but choosing the right interpolation search offers more tradeoffs. If you want the best algorithm for small datasets, you should choose $i-seq$. If you want to scale, then choose $i-guard$. If you want the best performance in between, then accelerate $i-seq$ with a faster linear search. If the conditions are right, then interpolation search far out-performs binary search. The choice is more difficult, however, because ultimately, interpolation search requires you to understand your data distribution and how large it is to achieve the best result.

\begin{figure}[t]
	\begin{tikzpicture}
	\begin{axis}[
	xtick=data,
	enlarge x limits={.2},
	legend pos=north west,
	title={Scaling with Dataset Size},
	xticklabels from table={szs-ns.dat}{sz},
	xlabel={Dataset Size},ylabel={Average Search Time (ns)},
	ybar
	]
	\addplot plot table[x expr=\coordindex, y=i-guard] {szs-ns.dat};
	\addlegendentry{$i-guard$}
	\addplot plot table[x expr=\coordindex, y=i-seq] {szs-ns.dat};
	\addlegendentry{$i-seq$}
	\addplot plot table[x expr=\coordindex, y=b-sz-lin] {szs-ns.dat};
	\addlegendentry{$b-sz-lin$}
	\end{axis}
	\end{tikzpicture}
\end{figure} \label{sizes}

\begin{figure}[t]
	\begin{tikzpicture}
	\begin{axis}[
    xtick=data,
    enlarge x limits={.2},
    legend pos=south east,
    ymin=0,
    title={Binary vs Interpolation Search},
    xticklabels from table={szs-ns-norm.dat}{sz},
    xlabel={Dataset Size},ylabel={Average Search Time / $b-sz-lin$ Search Time},
    ybar
	]
	\addplot plot table[x expr=\coordindex, y=i-guard] {szs-ns-norm.dat};
	\addlegendentry{$i-guard$}
	\addplot plot table[x expr=\coordindex, y=i-seq] {szs-ns-norm.dat};
	\addlegendentry{$i-seq$}

    \draw [black] ({rel axis cs:0,0}|-{axis cs:0,1}) -- ({rel axis cs:1,0}|-{axis cs:1,1}) node [pos=0.7, above] {$b-sz-lin$};
    \addlegendimage{line legend, color=black}
	\addlegendentry{$b-sz-lin$}
	\end{axis}
	\end{tikzpicture}
\end{figure} \label{compare}

\section{Threading}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
To determine if the number of concurrent threads impacts the performance, I initialize several thread threads to run the same search algorithm on the same permutation of data independently. Each thread records and stores its own samples. Finally, the results from each thread are concatenated together. For each number of concurrently running threads, dataset size, and algorithm, we record the change in median search time from its median search time with a single running thread. In this way, we can observe the way that multiple, simultaneously-running searches might interact with each other.

We consider $i-guard$, $i-seq$, and $b-sz-lin$ as in the evaluation across dataset sizes in section \ref{sizes}. We record the median performance of each algorithm on every dataset size and thread count, and normalize each algorithm's measurements against its single threaded performance. For each dataset size and thread count, we report the performance decrease of the algorithm that degraded most on that size that thread count. 

We show that with a pessimal choice of algorithm throughput does decrease, but the effect is small. CPU throttling is an important consideration. When a CPU gets hot, it will throttle the CPU frequency. This happens more frequently when all cores are in heavy use, so this may explain what contention we do observe. Each CPU core gets its own top-level cache, but the lower-level caches are shared. The lower-level caches only become a bottleneck when the dataset is hot if the dataset exceeds the size of the upper-level caches.

Another explanation of the performance degradation due to multiple threads is that the combined workset of all threads requires sharing the lower-level caches. In addition, the highest thread counts and the largest datasets record the most measurements, which may be a non-trivial amount of cache activity. These considerations may explain why the effect it strongest on the largest dataset which would rely upon the the larger, lower-level caches more due the size of its dataset. Note that each thread searches using the same random permutation, which may thrash the cache lesss than concurrent and independent queries.

\begin{figure}[t]
	\begin{tikzpicture}
	\begin{axis}[
	yticklabel={\pgfmathprintnumber\tick\%},
	bar width=6pt,
	ybar,
	width=\columnwidth,
	xtick=data,
	legend pos=north west,
	enlarge x limits={.2},
	title={Scaling with Threads},
	xticklabels from table={szs-thds-diff.dat}{n_thds},
    xlabel={Number of Threads},ylabel={Normalized Change in Performance}
	]
	\addplot plot table[x expr=\coordindex, y=1000] {szs-thds-diff.dat};
	\addlegendentry{$N = 10^3$}
	\addplot plot table[x expr=\coordindex, y=10000] {szs-thds-diff.dat};
	\addlegendentry{$N = 10^4$}
	\addplot plot table[x expr=\coordindex, y=100000] {szs-thds-diff.dat};
	\addlegendentry{$N = 10^5$}
	\addplot plot table[x expr=\coordindex, y=1000000] {szs-thds-diff.dat};
	\addlegendentry{$N = 10^6$}
	\end{axis}
	\end{tikzpicture}
\end{figure}

\section{Conclusion}
One of the challenges of presenting optimizations and trying to understand how they change program and its performance is that the ordering and combination of optimizations is such a significant factor. In section \ref{noeq} we described how the conditional move was saved only if both the overflow math was used and equality was not checked for. In the presented ordering, overflow math represents a relatively minor optimization, however, if overflow math had instead been presented later, several other optimizations would have been impacted and the different midpoint calculation would have been much more significant.

We have demonstrated many general techniques for optimization through re-structuring your code, and we presented how they apply specifically in the context of binary and interpolation search algorithms. We saw that in order to optimize our code, sometimes the intermediate steps actually worsened performance. This raises questions about the repeated advice to measure everything. While measurement certainly guided our process for optimizing these algorithms, measuring every incremental improvement can prevent you from escaping local maxima.

We focused on interpolation search in its preferred uniform data distribution, but we also showed how to determine if interpolation search is a good fit for your dataset by showing that the $L_1$ norm was predictive. We showed that interpolation search could out-perform binary search across a variety of dataset sizes, but that choosing the best interpolation search is difficult. Finally, we showed that concurrent searches could be run simultaneously without significantly degrading performance.

\onecolumn
\begin{thebibliography}{1}
\bibitem{perl-itai-avni} Perl, Itai, Avni. 1978.
\bibitem{gonnet-rogers} Gonnet, Rogers. 1977.
\bibitem{pvk-search-retro} Khuong, Paul. https://www.pvk.ca/Blog/2015/11/29/retrospective-on-binary-search-and-on-compression-slash-compilation/
\bibitem{pvk-binary-mispredictions} Khuong, Paul. pvk.ca/Blog/2012/07/03/binary-search-star-eliminates-star-branch-mispredictions/
\bibitem{pvk-binary-cache} Khuong, Paul. https://www.pvk.ca/Blog/2012/07/30/binary-search-is-a-pathological-case-for-caches/
\bibitem{jump-search} https://xlinux.nist.gov/dads/HTML/jumpsearch.html
\bibitem{linear-binary-search} Mostly Software. https://schani.wordpress.com/2010/04/30/linear-vs-binary-search/
\bibitem{carruth} Carruth, Chandler. Going Nowhere Faster. https://www.youtube.com/watch?v=2EWejmkKlxs
\bibitem{fish} http://ridiculousfish.com/blog/posts/labor-of-division-episode-i.html
\bibitem{demaine-jones-patrascu} E.D. Demaine, T.R. Jones, M. Patrascu Interpolation search for non-independent data SODA: ACM-SIAM Symposium on Discrete Algorithms (SODA), SIAM Press (2004), pp. 529-530
\bibitem{granlund-montgomery} Granlund, T., \& Montgomery, P. L. (1994, August). Division by invariant integers using multiplication. In ACM SIGPLAN Notices (Vol. 29, No. 6, pp. 61-72). ACM.
\bibitem{manolopoulos-kollias-burton} Manolopoulos, Y. P., Kollias, J. Y. G., \& Burton, F. W. (1987). Batched interpolation search. The Computer Journal, 30(6), 565-568.
\end{thebibliography}

\end{document}
