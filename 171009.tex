\documentclass{article}
\usepackage{pgfplots}
%\usepgfplotslibrary{statistics}

\title{Fast Search on Small, Uniformly Distributed Arrays}
\date{\today}

\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\begin{itemize}
  \item Goetz Graefe
  \item Willard and Mehlhorn / Tsakaldis
  \item Demaine
  \item Santoro-Sidney
  \item Bonasera, Ferrara, Fiumara, Pagano, Provetti 
  \item Interpolation Search
\end{itemize}
Interpolation search keeps track of an interval that could include the target. It projects the key into the interval by using the ratio between the key and the range of the remaining interval. It reduces the size of the interval by evaluating at that projected point.

Binary search keeps track of an interval that could include the target. It reduces the size of the interval by half by examining the element in the middle of the remaining interval.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Optimizations}
To measure performance of different search algorithms, I measure the time to search for a random permutation of elements in a sorted array of integers chosen uniformly at random. All performance measurements are given in terms of the data set that yielded median performance for $interpolation-linear$. For details, see the methodology.

A naive binary search includes two comparisons, non-uniform memory access and a branch that is likely to be unpredictable. When implemented on a sorted array, it loads an entire cache line to use a single element for the majority of its comparisons. The first variation that comes to mind is to check for equality only in the last step. While this requires an additional iteration on average, it can remove any branching on the comparison step.

\begin{figure}[h]
\begin{verbatim}
def binary-naive(x, A):
  left = 0
  right = len(A)
  while right > left:
    mid = left + (right-left) / 2
    if A[mid] < x:
      left = mid + 1
    elif A[mid] > x:
      right = mid 
    else
      return A[mid]
  return A[mid]
\end{verbatim}
\caption{binary-naive}
\end{figure}

This variant of binary search sets the left index and the right index conditionally on the result of the comparison, which requires two conditional moves. If the size of the array replaces the $right$ index, which changes unconditionally, then only a single conditional move is needed.

Choosing mid depends on a subtraction, an addition, and a shift. While only a sum is needed, this runs the risk of an overflow if the sum exceeds the width. Since left and right are indices, overflow is possible only with approximately $2^{63}$ elements for 8 byte integers.

Finally, without the equality check, the number of needed operations is defined to be in $\lceil log_2 n\rceil$. For a binary search that goes all the way to the end, you may be able to save an iteration on some of the operands, however, the gains will be even smaller if the binary search stops with a remaining interval left. As such, the number of needed iterations may be pre-computed which allows the use of the more friendly for loop structure. Together these improvements reduce search time by 57\%. (N)

\begin{figure}
\begin{verbatim}
def binary-size(x, A):
  left = 0
  n = len(A)
  while n > 1:
    n = n - n / 2
    mid = left + n / 2 
    if A[mid] <= x:
      left = left + n / 2
  return A[left]
\end{verbatim}
\caption{binary-size}
\end{figure}

A recursive interpolation search must pay the arithmetic cost of interpolation for each evaluation. To avoid overflow, it must either divide the distance by the reciprocal of the slope or use floating points to multiply the distance by the slope. Integer division is much more expensive than multiplication and requires two dependent division operations are necessary. Floating point arithmetic can be used, but this requires the conversion to floating point representation to interpolate, followed by a conversion to integers to index.

\begin{figure}
\begin{verbatim}
def interpolation-naive(x, A):
  left = 0
  right = len(A) - 1
  while left < right:
    width_range = (right - left) / (A[right] - A[left])
    mid = left = (x - A[left]) * width_range
    if A[mid] < x:
      left = mid + 1
    elif x < A[mid]:
      right = mid - 1
    else
      return A[mid]
  return A[left]
\end{verbatim}
\caption{interpolation-naive}
\end{figure}

The first interpolation is fixed, so the ratio of thw width to the range can be precomputed, decreasing search time by 49\%. (N) (Labeled interpolation-recurse.) The remaining interpolations must compute this dynamically since there will be a variety of different widths and ranges. Additionally, you can detect if the interpolation is progressing like a linear search and do the linear search directly. (All measurements of interpolation search include this improvement.)

If interpolation search operates much like a linear search after the first interpolation, and we should interpolate only so long as it poses an advantage over a linear search. However, before evaluating how this impacts interpolation search we should consider how to implement a linear search.

Without loss of generality, we can discuss $linear-naive$, a forward linear search. I leave adaptating to a reverse linear search as an exercise for the reader. (Needs description on what the linear search does.)

\begin{figure}
\begin{verbatim}
def linear-naive(x, A, left):
  while left < len(A) - 1 and A[left] < x:
    left = left + 1
  return A[left]
\end{verbatim}
\caption{linear-naive}
\end{figure}

 The first issue with this implementation is the additional comparison on every iteration to ensure bounds checking. By surrounding the array in a boundary of values that exceed the largest value in the dataset, we can remove this check since the end of the array will inform about array boundaries. Additionally, unrolling can be applied to the array.

This doesn't require special memory alignment to be fast, and it perfoms the best of any linear search I've found up to ~40 elements (N). However, longer searches will saturate the pipeline and the front-end, so to achieve high performance for longer searches, some vectorization may be in order. I found that the fastest AVX searches (on unaligned seeks) started with an unrolled loop similar to linear-unroll. This guarantees that any element that is nearby the seek value will be found in the minimum of time. I found it to perform better than unaligned vectorized searches or only doing scalar comparisons until alignment was achieved.

\begin{figure}
\begin{verbatim}
# roll is the unroll count
def linear-unroll(x, A, left, unroll):
  while True:
    for i in 0 .. unroll:
      if A[left + i] >= x:
        return left + i
    left = left + unroll
\end{verbatim}
\caption{linear-unroll}
\end{figure}

The principles of the SIMD search are to use an unrolled search for the first few elements, to load a group of consecutive vectors of elements and compare them simultaneously, and to skip the full comparison of the group of vectors if possible by doing an early comparison on the last one. (Note this is using the term linear search loosely since some comparisons are skipped by a stride width.) The early unrolled search achieves alignment, but it can be beneficial to use the unrolled search a little further, trading off significant gains in short search speed for small costs in long search speed. Loading a group of consecutive vectors allows for parallelism of comparisons within a vector and maximizing the pipeline across vectors. Once the vector that contains the element has been detected, the comparison mask can be transformed into an index by compressing it to an integer, counting the number of bits in the suffix of the mask, and some arithmetic. [inspired by https://schani.wordpress.com/2010/04/30/linear-vs-binary-search/]

\begin{figure}
\begin{verbatim}
# unroll is a multiple of vecWidth
# implementation details may vary to use vectorization
def linear-simd(x, A, left, unroll, vector_count, vector_width):
  for unroll_index in 0 .. unroll:
    if A[left + unroll_index] <= x:
      return left + unroll_index
  misalignment = A + left \mod vector_width
  left = left + unroll - misalignment
  while True:
    last = left + vector_width * (vector_count - 1)
    last_vector = A[last .. last + vector_width - 1]
    if none([a > x for a in last_vector]]):
      continue
    for vector_index in 0 .. vector_count:
      mask = [a > x for a in A[left .. left + vector_width - 1]]
      if any(mask):
        tzcount = min([i if A[i] == 1 else 0
                       for i in 0 .. len(A) - 1])
        return left + tzcount / 8 + 4 * vector_index
\end{verbatim}
\caption{linear-simd}
\end{figure}

If an element is nearby, then it is much faster to find it using the above linear searches compared to recursively interpolating.  In addition to saving on the arithmetic for interpolation, which is much more expensive after the first, pre-computation assisted interpolation, memory access becomes uniform and comparisons can take advantage of the superscalar processor. So, if interpolation search is effective, recursively interpolating might be effective, but linear search will also be effective and much cheaper. If interpolation search is not effective, then further interpolating is unlikely to help, and linear search will at least iterate more quickly.  A single interpolation followed by a linear search reduces search time by 72\% (N) relative to the optimized recursive variant.

\begin{figure}
\begin{verbatim}
def interpolation-linear-fp(x, A, width_range):
  width_range = (len(A)-1) / (A[len(A) - 1] - A[0])
  mid = (x - A[0]) * width_range
  if A[mid] > x:
    return A[linear-backward(A. mid - 1, x)]
  else:
    return A[linear-forward(A, mid, x)]
\end{verbatim}
\caption{interpolation-linear-fp}
\end{figure}

The use of floating point arithmetic still requires conversions to and from integers, but precise integer division is too expensive. However, interpolation is an approximate process and integer division truncates, so an approximate division will not be much worse in the average case. Modern compilers will replace division by a known integer constant by up to a multiplication, subtraction, 2 shifts, and an add or some subset thereof. This runs faster than a single integer division, and the error is bounded such that it is indistinguishable for the truncated integer division. However, a very close approximation, exact for some divisors, can be obtained by just the multiplication. An approximation of the reciprocal of the divisor can accelerate the first interpolation, reducing search time by 26\%. (N) (Labeled interpolation-linear.) 

I have discussed several techniques to optimize  binary, interpolation, and linear search. Linear search was accelerated using guards, unrolling and vector instructions. Binary search was acclerated by reducing the number of branches and conditional moves, and with some use of linear search. (Labeled binary-linear.)  Interpolation search was accelerated by precomputation, replacing division with multiplication, and with the uniform memory access of linear search.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methodology}
One of the admirable traits about binary search is that its performance is independent of the values of the data. While interpolation search increases efficiency by taking advantage of knowledge about the data distribution, this means that the performance of interpolation search will vary depending on the properties of the data. To simulate keys that form a uniform distribution, I generated sorted lists of 1000 integers chosen uniformly at random in $[1, 2^{63} - 1]$ using numpy.

To measure the performance of each kind of search and each variant, I use a custom microbenchmark harness. I took care to ensure that the overhead of the harness is minimized. By taking a sum over the return values of each search, I ensure that the searches aren't optimized out at minimal overhead. This allows for a simple, low-overhead, run-time verification of the correctness of the search algorithms.

Using a fixed seed to ensure repeatability, a pseudo-random permutation of the values in the list is generated. Each element in the list is searched for exactly once, and its value is added to a sum that is used for verification. Because the values are searched for randomly, this prevents artificial locality and branch predictability that is unrealistic in real workloads. The number of cycles to complete all searches is recorded and the correctness of the searches is verified. Run times are stable after 2,000 trials, so the first 2,000 are discarded, and the standard deviation and mean are calculated over the remaining 3,000 trials. This process is completed for each of the 30 different random lists of numbers. With this process, the maximum standard deviation is less that (N) across all measurements.

Linear search was benchmarked separately by using Google's benchmark framework because I was interested in understanding the relationship between the length of the search and the time it took for a particular implementation. A random permutation of searches was again used, but the distance to the left or right is held fixed. This prevents unrealistic locality, but allows different distances to be treated differently.

(Bonasera, Demaine?) suggest that smoothness is a property that predicts the suitability of interpolation search for a set of numbers. Smoothness is defined to be the ratio between the greatest and least distances between two adjacent values in the list of numbers. Using python and pandas, I examine several different metrics to see how well they predict the runtime performance of my fastest interpolation search. I consider smoothness against $L_1$ and $L_2$ norms as computed against a line of best fit and as computed against a line connecting the extremes of the dataset.

\begin{itemize}
  \item Show hardware counter breakdown on interpolation and binary search.
  \item Include information about the run environment and addendums about the impact of compilers, etc.
  \item Consider discussion of compilers.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}
For $interpolation-linear$, the cost of a given search is driven by three factors. First, the cost of the interpolation itself. Second, the distance from the interpolation to the correct location. Third, the relationship between that distance and the average time to search each element in that distance. (There are economies of scale in vectorized linear searches.)

I have plotted a cumulative histogram showing the distance between a linear interpolation and the correct index on the dataset with median performance. I have also plotted the mean, 90\%, and maximum distances across all 30 datasets sorted by the mean distance. This shows a weak relationship between the mean distance and a weak, but still stronger relationship between maximum and 90\% distance.

\begin{tikzpicture}
  \begin{axis}[
    title={Interpolation Distance, Median},
    xlabel={Distance},
    ylabel={Count},
    ybar,
    ymin=0
  ]
    \addplot +[
      hist={
        cumulative,
        density,
        bins=7
      }] table [y index=0] {cdf.dat};
  \end{axis}
\end{tikzpicture}

$linear-simd$ can be parameterized by the number of groups to unroll. Values of 1 and 2 are interesting because 2 provides a useful point in the tradeoff between short searches and longer ones while 1 yields the best performance for the longest searches. $linear-unroll$ can be parameterized by the unroll factor, however, I found that a value of 8 was better than all other measured values in this experiment, so I show the results only for that value and for the case without unrolling.

%%%%%%  lin.dat %%%%%

I have plotted the performance of $interpolation-linear$, $binary-linear$, and an $oracle$ search to provide perspective on the lower-limit of performance. The $oracle$ search is defined to have been given prior knowledge of the sequence of searches, so it will simply look up which index is next to arrive, ensure that the value is correct, and return it. All plots have been sorted by the performance of $interpolation-linear$.

\begin{tikzpicture}
  \begin{axis}[
      title={Binary vs Interpolation Search, All Datasets},
      xlabel={Dataset},
      ylabel={Time relative to oracle},
      legend style={at={(0.5,0)},anchor=north},
      legend columns=3,
      xmajorticks=false
    ]
    \addplot table [y=binary-linear,x=file]{bin-int.dat};
    \addlegendentry{$binary-linear$}
    \addplot table [y=interpolation-linear,x=file]{bin-int.dat};
    \addlegendentry{$interpolation-linear$}
  \end{axis}
\end{tikzpicture}

\begin{tikzpicture}
  \begin{axis}[
	  title={Algorithm Performance, Median Dataset},
	  ylabel={Time relative to oracle},
	  xticklabels from table={median.dat}{algorithm},
	  xtick=data,
	  x tick label style={rotate=90},
	  ybar,
    % fit chart
    ymin=0,
    enlarge y limits={upper, value=.1},
    bar width=32pt,
    % draw labels
    nodes near coords,
    % min ink
    yticklabels={\empty},
    ytick style={draw=none},
    axis lines*=left,
    y axis line style={opacity=0}
	  ]
  \addplot table[x expr=\coordindex ,y=cycles] {median.dat};
  \end{axis}
\end{tikzpicture}

After computing each metric on each dataset, I plot the performance of $interpolation-linear$ on each dataset against the corresponding metric value. Then, I consider the $R^2$ value of the line of best fit on that plot. We can see that $L_1$ is strongly predictive, $L_2$ is weakly predictive, and that smoothness does not correlate with the performance of interpolation search in this context.

\begin{tikzpicture}
\begin{axis}[
	title={$L_1$, 90\%, $L_\infty$, All Datasets},
  xlabel=Dataset,ylabel={Error},
  legend style={at={(0.5,0)},anchor=north},
  legend columns=3,
	xmajorticks=false
	]
\addplot table[x=dataset,y=l1]{metric.dat};
\addlegendentry{$L_1$}
\addplot table[x=dataset,y=90]{metric.dat};
\addlegendentry{90\%}
\addplot table[x=dataset,y=max]{metric.dat};
\addlegendentry{$L_\infty$}
\end{axis}
\end{tikzpicture}

\begin{tikzpicture}
\begin{axis}[
	title={Metric Prediction of Performance},
	xlabel=Metric,ylabel={$R^2$},
	xticklabels from table={predict.dat}{metric},
	xtick=data,
	ybar
	]
\addplot table[x expr=\coordindex, y=r2] {predict.dat};
\end{axis}
\end{tikzpicture}

\begin{itemize}
  \item linear-unroll-8, linear-simd-1, linear-simd-2, linear-naive across distances
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
\begin{itemize}
  \item Interpolation search is interesting for small arrays.
  \item Integration into B-Tree
  \item Development of Interpolation Search Tree
  \item Investigation into splines
  \item Other distributions? Adaptability? Keys with more costly comparisons or additional records that make linear search worse. Workset size?
\end{itemize}
\begin{itemize}
  \item Interpolation / Binary search hybrid?
\end{itemize}

\end{document}
